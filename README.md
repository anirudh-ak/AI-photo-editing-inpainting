# AI Photo Editing with Inpainting

This project showcases an AI-driven photo editing tool that allows you to swap out the background of a subject and replace it with an image generated by Stable Diffusion using a text prompt. The project leverages state-of-the-art models for inpainting and image generation to create seamless and realistic edits.

## Table of Contents

- [Introduction](#introduction)
- [Installation](#installation)
- [Usage](#usage)
- [Models Used](#models-used)
- [Contributing](#contributing)

## Introduction

In this project, we build a web application that enables users to perform AI-driven photo editing by swapping the background of an image with a new one generated using Stable Diffusion. The process involves:

1. Segmenting the subject from the background using the SAM model (Segment Anything Model).
2. Generating a new background using a text prompt with the Stable Diffusion model.
3. Merging the subject with the newly generated background to create a seamless composite image.


## Installation

To run this project, you'll need to install the necessary dependencies. The primary dependencies include `PIL`, `requests`, `transformers`, and `diffusers`. You can install them using `pip`:

```bash
pip install pillow requests transformers diffusers torch numpy
```

## Usage
Clone this repository:

```bash
git clone https://github.com/your-username/ai-photo-editing-inpainting.git
cd ai-photo-editing-inpainting
```

- Run the notebook:

You can open the provided Jupyter notebook in your preferred environment and follow the steps to perform photo editing.

- Web Application:

This project can be extended to create a web application where users can upload their images, enter text prompts, and download the edited images.

## Notebook Steps
- Load Required Libraries: Import the necessary libraries, including PIL, requests, transformers, and diffusers.
- Load the SAM Model: Load the SAM model from Facebook/Meta and prepare it for image segmentation.
- Generate Images: Use Stable Diffusion to generate new backgrounds based on text prompts.
- Inpainting: Use the inpainting model to merge the subject with the newly generated background.

## Models Used

- SAM (Segment Anything Model): Used for segmenting the subject from the background.
- Stable Diffusion: A state-of-the-art model for generating images from text prompts.
- Inpainting Model: Merges the segmented subject with the newly generated background.

## Contributing

Contributions are welcome! If you find any issues or have suggestions for improvements, feel free to open an issue or submit a pull request.
